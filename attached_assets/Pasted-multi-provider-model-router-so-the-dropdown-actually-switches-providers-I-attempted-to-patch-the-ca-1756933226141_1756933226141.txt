multi-provider model router so the dropdown actually switches providers. I attempted to patch the canvas inline, but the bulk update failed. No worries: here’s the exact code you can paste into your project (and/or I can re-insert into the canvas in smaller chunks if you prefer).

1) Add a provider router

Create: lib/providers.ts

export type ProviderID = "openai" | "anthropic" | "google" | "xai" | "cohere" | "router";

export interface ProviderCall {
  provider: ProviderID;
  model: string; // provider-native name, e.g. "gpt-4o-mini" | "claude-3-7-sonnet" | "gemini-2.5-pro"
  messages: { role: string; content: any }[];
  temperature?: number;
}

// --- OPENAI (uses your existing lib/openai.ts stream) ---
import { openAIStream } from "@/lib/openai";
export async function* streamOpenAI(p: ProviderCall): AsyncGenerator<string> {
  yield* openAIStream(p.model, p.messages, p.temperature ?? 0.7);
}

// --- ANTHROPIC (Claude) ---
export async function* streamAnthropic(p: ProviderCall): AsyncGenerator<string> {
  const res = await fetch("https://api.anthropic.com/v1/messages", {
    method: "POST",
    headers: {
      "content-type": "application/json",
      "x-api-key": process.env.ANTHROPIC_API_KEY || "",
      "anthropic-version": "2023-06-01",
    },
    body: JSON.stringify({
      model: p.model,
      max_tokens: 4096,
      temperature: p.temperature ?? 0.7,
      messages: p.messages,
      stream: true,
    }),
  });
  if (!res.ok || !res.body) throw new Error("Anthropic request failed");

  const reader = res.body.getReader();
  const dec = new TextDecoder();
  while (true) {
    const { value, done } = await reader.read();
    if (done) break;
    const chunk = dec.decode(value);
    for (const line of chunk.split("\n")) {
      if (!line.startsWith("data: ")) continue;
      const payload = line.slice(6).trim();
      if (!payload || payload === "[DONE]") continue;
      try {
        const evt = JSON.parse(payload);
        if (evt.type === "content_block_delta" && evt.delta?.text) yield evt.delta.text as string;
      } catch {}
    }
  }
}

// --- GOOGLE (Gemini) ---
export async function* streamGemini(p: ProviderCall): AsyncGenerator<string> {
  const key = process.env.GOOGLE_API_KEY || "";
  const url = `https://generativelanguage.googleapis.com/v1beta/models/${p.model}:streamGenerateContent?key=${key}`;
  const res = await fetch(url, {
    method: "POST",
    headers: { "content-type": "application/json" },
    body: JSON.stringify({
      contents: [
        {
          role: "user",
          parts: [{ text: p.messages.find(m => m.role === "user")?.content || "" }],
        },
      ],
      generationConfig: { temperature: p.temperature ?? 0.7 },
    }),
  });
  if (!res.ok || !res.body) throw new Error("Gemini request failed");

  const reader = res.body.getReader();
  const dec = new TextDecoder();
  while (true) {
    const { value, done } = await reader.read();
    if (done) break;
    const s = dec.decode(value);
    try {
      const lines = s.trim().split("\n");
      for (const l of lines) {
        const j = JSON.parse(l);
        const txt = j.candidates?.[0]?.content?.parts?.[0]?.text;
        if (txt) yield txt;
      }
    } catch {}
  }
}

// --- xAI (Grok) ---
export async function* streamGrok(p: ProviderCall): AsyncGenerator<string> {
  const res = await fetch("https://api.x.ai/v1/chat/completions", {
    method: "POST",
    headers: {
      "content-type": "application/json",
      Authorization: `Bearer ${process.env.XAI_API_KEY || ""}`,
    },
    body: JSON.stringify({ model: p.model, messages: p.messages, stream: true }),
  });
  if (!res.ok || !res.body) throw new Error("xAI request failed");
  const reader = res.body.getReader();
  const dec = new TextDecoder();
  while (true) {
    const { value, done } = await reader.read();
    if (done) break;
    const chunk = dec.decode(value);
    for (const line of chunk.split("\n")) {
      if (!line.startsWith("data: ")) continue;
      const payload = line.slice(6).trim();
      if (payload === "[DONE]") return;
      try {
        const j = JSON.parse(payload);
        const delta = j.choices?.[0]?.delta?.content;
        if (delta) yield delta;
      } catch {}
    }
  }
}

// --- Cohere (chunked fallback) ---
export async function* streamCohere(p: ProviderCall): AsyncGenerator<string> {
  const res = await fetch("https://api.cohere.ai/v2/chat", {
    method: "POST",
    headers: { "content-type": "application/json", Authorization: `Bearer ${process.env.COHERE_API_KEY || ""}` },
    body: JSON.stringify({ model: p.model, messages: p.messages }),
  });
  if (!res.ok) throw new Error("Cohere request failed");
  const data = await res.json();
  const text = data?.message?.content?.map((x: any) => x.text).join("") || "";
  for (let i = 0; i < text.length; i += 120) yield text.slice(i, i + 120);
}

// --- Main Router by modelId prefix: "provider:modelName"
export async function* streamByModelId(
  modelId: string,
  messages: { role: string; content: any }[],
  temperature = 0.7
): AsyncGenerator<string> {
  const [provider, model] = modelId.split(":", 2) as [ProviderID, string];
  const call: ProviderCall = { provider, model, messages, temperature };
  switch (provider) {
    case "openai":     return yield* streamOpenAI(call);
    case "anthropic":  return yield* streamAnthropic(call);
    case "google":     return yield* streamGemini(call);
    case "xai":        return yield* streamGrok(call);
    case "cohere":     return yield* streamCohere(call);
    case "router": {
      // trivial router example: long prompts → Claude, else GPT
      const text = (messages.find(m => m.role === "user")?.content || "") as string;
      const prefer = text.length > 1200 ? "anthropic:claude-3.7-sonnet" : "openai:gpt-4o-mini";
      return yield* streamByModelId(prefer, messages, temperature);
    }
    default:
      throw new Error(`Unknown provider: ${provider}`);
  }
}

2) Use the router in your chat API

Edit: app/api/chat/send/route.ts

import { streamText } from "@/lib/sse";
import { streamByModelId } from "@/lib/providers";

export const runtime = "nodejs";

export async function POST(req: Request) {
  try {
    const body = await req.json();
    const { prompt, mode, action, model = "openai:gpt-4o-mini", temperature = 0.7 } = body;

    // handle special actions (image/research/ppt) as you already do…

    const gen = streamByModelId(
      model,
      [
        { role: "system", content: systemForMode(mode) },
        { role: "user", content: prompt },
      ],
      temperature
    );

    return streamText(gen);
  } catch (e: any) {
    return new Response(JSON.stringify({ error: e.message }), { status: 500 });
  }
}

function systemForMode(mode: string) {
  switch (mode) {
    case "deepAgent":
      return "You are DeepAgent. Plan multi-step solutions, list steps, then execute each step conceptually and summarize results.";
    case "study":
      return "You are Study Mode. Teach concepts clearly, add quick quizzes, and suggest follow-up reading.";
    default:
      return "You are a helpful AI assistant.";
  }
}

3) Ensure the selector sends a proper modelId

Your ModelSelector is already returning IDs like "openai:gpt-5" or "router:RouteLLM". Keep ChatComposer_v2’s model state as that string and pass it to /api/chat/send.

4) Env keys to add

.env

OPENAI_API_KEY=...
ANTHROPIC_API_KEY=...
GOOGLE_API_KEY=...
XAI_API_KEY=...
COHERE_API_KEY=...
SERPER_API_KEY=...   # for Deep Research

5) Test checklist (quick)

Select GPT-5 (openai:*), send “Hello” → response streams.

Select Claude Sonnet 4 (anthropic:*), send “Hello” → response streams.

Select Gemini 2.5 Pro (google:*), send “Hello” → response streams.

Select Grok 4 (xai:*), send “Hello” → response streams.

Select RouteLLM (router:RouteLLM), send a short vs long prompt and confirm routing switches (watch logs if needed).